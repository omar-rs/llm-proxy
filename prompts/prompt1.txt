write me a simple llm proxy server in golang that would take a llm call like the one in sample_openai_chat_completions_request.txt,
along with a request header of "Posit-Client-Type:positron-assistant" and proxy it to openai's chat completions API.

the proxy server should:
- run on port 8081 and have a single endpoint /v1/chat/completions that accepts POST requests in JSON format
- call openai using an openai API key from a .env file that you will create
- strip any headers in the original request that start with "Posit"
- return the streaming response in chunks that don't exceed 1 line.
- extract usage information out of the llm call and summarize the input/output tokens used and print it to the console
- print out the final response from openai to the console

use /Users/omar/work/llm-request-response-demo/llm_demo.py for inspiration and reference on how to make calls to
openai's chat completions api and how to handle streaming responses and usage.

write a test for the proxy server in python that would invoke the proxy server's endpoint and print the chunked response.
